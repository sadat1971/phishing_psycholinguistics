{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Saving the files in pkl format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iwspa train and test is just copied and pasted. SpamSMS is processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                               text\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/disk2/sadat/FakeNewsData/archive/spam.csv\", encoding = 'latin-1')\n",
    "df.rename(columns={\"v1\":\"labels\", \"v2\":\"text\"}, inplace=True)\n",
    "spam = df[[\"labels\", \"text\"]]\n",
    "#spam = spam.sample(frac=1, random_state=100).reset_index(drop=True)\n",
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-1165b68f4273>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spam[\"is_phishing\"] = spam[\"labels\"].apply(lambda x:1 if x==\"spam\" else 0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "      <th>is_phishing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                               text  is_phishing\n",
       "0    ham  Go until jurong point, crazy.. Available only ...            0\n",
       "1    ham                      Ok lar... Joking wif u oni...            0\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...            1\n",
       "3    ham  U dun say so early hor... U c already then say...            0\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...            0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam[\"is_phishing\"] = spam[\"labels\"].apply(lambda x:1 if x==\"spam\" else 0)\n",
    "spam.to_pickle(\"/disk2/sadat/PhishingResearch/processed_work/psych_trait_experiments/Files/spamSMS.pkl\")\n",
    "spam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "def tokenization_all_datasets(path, one_file_only=False):\n",
    "    start = time.time()\n",
    "    if one_file_only==False:\n",
    "        all_files = os.listdir(path)\n",
    "        for files in all_files:\n",
    "            df = pd.read_pickle(path + files)\n",
    "            print(files)\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "            df[\"tokenized\"] = df[\"text\"].apply(lambda sent:tokenizer.encode(sent, add_special_tokens=True, \n",
    "                                                                                  max_length=512, truncation=True,\n",
    "                                                                                  padding='max_length', \n",
    "                                                                                  return_attention_mask=False))\n",
    "            df.to_pickle(path + files)\n",
    "            end = time.time()\n",
    "            elapsed = end-start\n",
    "            x = files + \"is_done in \" + str(\"%.2f\" %(elapsed)) + \" seconds\\n\"\n",
    "    else:\n",
    "        df = pd.read_pickle(path)\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "        df[\"tokenized\"] = df[\"text\"].apply(lambda sent:tokenizer.encode(sent, add_special_tokens=True, \n",
    "                                                                              max_length=512, truncation=True,\n",
    "                                                                              padding='max_length', \n",
    "                                                                              return_attention_mask=False))\n",
    "        df.to_pickle(path)\n",
    "        end = time.time()\n",
    "        elapsed = end-start        \n",
    "\n",
    "tokenization_all_datasets(\"/disk2/sadat/PhishingResearch/processed_work/psych_trait_experiments/Files/spamSMS.pkl\", \n",
    "                          one_file_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"/disk2/sadat/PhishingResearch/processed_work/psych_trait_experiments/Files/spamSMS.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "      <th>is_phishing</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 2175, 2127, 18414, 17583, 2391, 1010, 46...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 7929, 2474, 2099, 1012, 1012, 1012, 1664...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2489, 4443, 1999, 1016, 1037, 1059, 2243...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 1057, 24654, 2360, 2061, 2220, 7570, 209...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 20976, 1045, 2123, 1005, 1056, 2228, 200...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                               text  is_phishing  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...            0   \n",
       "1    ham                      Ok lar... Joking wif u oni...            0   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...            1   \n",
       "3    ham  U dun say so early hor... U c already then say...            0   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...            0   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [101, 2175, 2127, 18414, 17583, 2391, 1010, 46...  \n",
       "1  [101, 7929, 2474, 2099, 1012, 1012, 1012, 1664...  \n",
       "2  [101, 2489, 4443, 1999, 1016, 1037, 1059, 2243...  \n",
       "3  [101, 1057, 24654, 2360, 2061, 2220, 7570, 209...  \n",
       "4  [101, 20976, 1045, 2123, 1005, 1056, 2228, 200...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Saving BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_set(datapath):\n",
    "    df = pd.read_pickle(datapath)\n",
    "    train_text = np.array(list(df[\"tokenized\"]))\n",
    "    attention_masks = np.where(train_text>0, 1, 0)\n",
    "    return torch.tensor(train_text), torch.tensor(attention_masks)\n",
    "\n",
    "def create_dataloader(train_text, attention_masks, batch_size=512):\n",
    "    # Create the DataLoader for our training set\n",
    "    '''\n",
    "    This function will create a dataloader for our training set. The dataloader will help to feed the randomly \n",
    "    sampled data on each batch. The batch size is selected to be 16, is simply as instructed in the original\n",
    "    paper. \n",
    "    '''\n",
    "    train_data = TensorDataset(train_text, attention_masks)\n",
    "    train_sampler = SequentialSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    return train_dataloader\n",
    "\n",
    "def BERT_features_saved(datapath):\n",
    "    for filename in [f for f in os.listdir(datapath) if f.endswith(\".pkl\")]:\n",
    "        input_ids, att_mask = create_data_set(datapath+filename)\n",
    "        device = torch.device(\"cuda\")\n",
    "        bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "        loader = create_dataloader(input_ids, att_mask, batch_size=16)\n",
    "        All_batch = []\n",
    "        for step, batch in enumerate(loader):\n",
    "            token, mask = batch\n",
    "            outputs = bert_model(token.to(device), mask.to(device))\n",
    "            last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "            np_array = last_hidden_state_cls.detach().cpu().numpy()\n",
    "            All_batch.append(np_array)\n",
    "            if step%100==0:\n",
    "                print(\"{} completed {} percent\".format(filename, step*100/len(loader)))\n",
    "                      \n",
    "        p = np.concatenate(All_batch, axis=0)\n",
    "        npy_file_path = datapath + \"BERT_\" + filename.split(\".pkl\")[0] + \".npy\"\n",
    "        saved_path = np.save(npy_file_path, p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spamSMS.pkl completed 0.0 percent\n",
      "spamSMS.pkl completed 28.653295128939828 percent\n",
      "spamSMS.pkl completed 57.306590257879655 percent\n",
      "spamSMS.pkl completed 85.95988538681948 percent\n",
      "combined_iwspa_ap_train_set.pkl completed 0.0 percent\n",
      "combined_iwspa_ap_train_set.pkl completed 27.932960893854748 percent\n",
      "combined_iwspa_ap_train_set.pkl completed 55.865921787709496 percent\n",
      "combined_iwspa_ap_train_set.pkl completed 83.79888268156425 percent\n",
      "iwspa_ap_test.pkl completed 0.0 percent\n",
      "iwspa_ap_test.pkl completed 37.174721189591075 percent\n",
      "iwspa_ap_test.pkl completed 74.34944237918215 percent\n"
     ]
    }
   ],
   "source": [
    "BERT_features_saved(\"/disk2/sadat/PhishingResearch/processed_work/psych_trait_experiments/Files/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 1.2.0, however, your version is 1.0.3. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.3 s, sys: 504 ms, total: 7.81 s\n",
      "Wall time: 2.25 s\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('paraphrase-mpnet-base-v2').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = model.encode(spam[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SBERT_features_saved(datapath):\n",
    "    for filename in [f for f in os.listdir(datapath) if f.endswith(\".pkl\")]:\n",
    "        df = pd.read_pickle(datapath + filename)\n",
    "        device = torch.device(\"cuda\")\n",
    "        sbert_model = SentenceTransformer('paraphrase-mpnet-base-v2').to(device)\n",
    "        sentence_embeddings = sbert_model.encode(df[\"text\"])\n",
    "        npy_file_path = datapath + \"SBERT_\" + filename.split(\".pkl\")[0] + \".npy\"\n",
    "        saved_path = np.save(npy_file_path, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 1.2.0, however, your version is 1.0.3. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "You try to use a model that was created with version 1.2.0, however, your version is 1.0.3. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "You try to use a model that was created with version 1.2.0, however, your version is 1.0.3. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SBERT_features_saved(\"/disk2/sadat/PhishingResearch/processed_work/psych_trait_experiments/Files/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
